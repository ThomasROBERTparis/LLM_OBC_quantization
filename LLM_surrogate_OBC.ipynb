{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MVA 24/25 - LLM Final Project - On LLM Quantization**\n",
    "\n",
    "# Optimal Brain Quantization\n",
    "\n",
    "Samson GOUREVITCH, Thomas ROBERT\n",
    "\n",
    "$\\texttt{\\{samson.gourevitch,thomas.robert.x21\\}@polytechnique.edu}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets, transformers\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'  # Example, check actual availability\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "unquantized_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "unquantized_model = unquantized_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"qnli\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fine-tuning on Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "train_batch_size = 64\n",
    "eval_batch_size = 64\n",
    "\n",
    "optimizer = torch.optim.AdamW(unquantized_model.parameters(), lr=learning_rate)\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=train_batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=eval_batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    unquantized_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    ### Training\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = unquantized_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    ### Evaluation\n",
    "    unquantized_model.eval()\n",
    "    eval_loss = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = unquantized_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_accuracy = accuracy_score(true_labels, preds)\n",
    "    print(f\"Epoch {epoch+1}, Eval Loss: {avg_loss:.4f}, Eval Loss: {avg_eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate OBC Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(weights, grid):\n",
    "  \"\"\"Quantize a weight to the nearest value in the quantization grid.\"\"\"\n",
    "  return torch.round(weights / grid) * grid\n",
    "\n",
    "def naive_quantization(W, quantization_grid):\n",
    "  return quantize(W, quantization_grid)\n",
    "\n",
    "def compute_H(X):\n",
    "  \"\"\"Compute the average Hessian matrix H_F = 2 * (X_reshaped^T * X_reshaped) / N,\n",
    "  where N is the total number of vectors from X.\n",
    "  Assumes X is a tensor with shape [batch_size, seq_length, hidden_size].\n",
    "  \"\"\"\n",
    "  # Reshape to merge batch and sequence dimensions.\n",
    "  X_reshaped = X.view(-1, X.shape[-1])\n",
    "  N = X_reshaped.size(0)\n",
    "  H_avg = 2 * (X_reshaped.T @ X_reshaped) / N\n",
    "  return H_avg\n",
    "\n",
    "def compute_H_inv(H, damp=1e-3):\n",
    "  \"\"\"Compute the inverse of the Hessian matrix H_F.\n",
    "  Adds a small damping factor to the diagonal for numerical stability.\n",
    "  \"\"\"\n",
    "  I = torch.eye(H.shape[0], device=H.device, dtype=H.dtype)\n",
    "\n",
    "  L = torch.linalg.cholesky(H + damp * I)\n",
    "  L_inv = torch.inverse(L)\n",
    "  H_inv = L_inv.T @ L_inv\n",
    "\n",
    "  return H_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_OBC(W,X, quantization_grid):\n",
    "  \"\"\"Optimal Brain Quantization (OBQ) \"\n",
    "      W: weight matrix\"\n",
    "      X: input vector\"\n",
    "      quantization_grid: quantization precision\"\n",
    "  \"\"\"\n",
    "  H = compute_H(X)\n",
    "  H_inv_save = compute_H_inv(H)\n",
    "\n",
    "  Q = torch.zeros_like(W)\n",
    "\n",
    "  for row in tqdm(range(W.shape[0])):\n",
    "    w = W[row,:]\n",
    "    H_inv = H_inv_save.clone()\n",
    "\n",
    "    for col in range(W.shape[1]):\n",
    "      q = quantize(w[col], quantization_grid)\n",
    "      Q[row, col] = q\n",
    "\n",
    "      error = w[col] - q\n",
    "      d = torch.diag(H_inv)[col]\n",
    "\n",
    "      Hinv_row = H_inv[col,]\n",
    "\n",
    "      w = w - error  / d * (Hinv_row)\n",
    "      H_inv = H_inv - torch.outer(Hinv_row, Hinv_row) / d\n",
    "\n",
    "  return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach quantization hooks to perform quantization during the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_OBC = copy.deepcopy(unquantized_model).to(device)\n",
    "model_naive = copy.deepcopy(unquantized_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hook(module, input, output):\n",
    "    module.__dict__[\"unquantized_input\"] = input\n",
    "    module.__dict__[\"unquantized_output\"] = output\n",
    "    # module.__dict__[\"unquantized_weight\"] = module.weight\n",
    "    \n",
    "    module._forward_hooks.popitem()\n",
    "\n",
    "save_hooks = []\n",
    "\n",
    "for layer_idx, layer in enumerate(model_OBC.bert.encoder.layer):\n",
    "    save_hooks.append(layer.attention.self.query.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.attention.self.key.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.attention.self.value.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.attention.output.dense.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.intermediate.dense.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.output.dense.register_forward_hook(save_hook))\n",
    "\n",
    "for layer_idx, layer in enumerate(model_naive.bert.encoder.layer):\n",
    "    save_hooks.append(layer.attention.self.query.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.attention.self.key.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.attention.self.value.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.attention.output.dense.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.intermediate.dense.register_forward_hook(save_hook))\n",
    "    save_hooks.append(layer.output.dense.register_forward_hook(save_hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Calibration data\n",
    "calibration_batch_size = 64\n",
    "calibration_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=calibration_batch_size, shuffle=True)\n",
    "\n",
    "calibration_batch = next(iter(calibration_dataloader))\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "# Forward pass through the model to trigger the hooks with calibration data from\n",
    "with torch.no_grad():\n",
    "  print(\"Saving input/output for unquantized version to OBC model\")\n",
    "  outputs = model_OBC(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "  print(\"Saving input/output for unquantized version to naive model\")\n",
    "  outputs = model_naive(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_grid = 0.001\n",
    "\n",
    "weight_approximation_error_OBC = list()\n",
    "reconstruction_error_OBC = list()\n",
    "def OBC_hook(module, input, output, quantization_grid=quantization_grid):\n",
    "    X_data = module.unquantized_input[0]\n",
    "    W = module.weight.clone()\n",
    "    quantized_weight = quantize_OBC(W, X_data, quantization_grid=quantization_grid)\n",
    "    module.weight = nn.Parameter(quantized_weight)\n",
    "    \n",
    "    module._forward_hooks.popitem()\n",
    "    new_output = module(*input)\n",
    "\n",
    "    weight_approximation_error_OBC.append(F.mse_loss(W,module.weight))\n",
    "    reconstruction_error_OBC.append(F.mse_loss(module.unquantized_output, new_output))\n",
    "\n",
    "    del module.unquantized_input\n",
    "    del module.unquantized_output\n",
    "\n",
    "\n",
    "\n",
    "weight_approximation_error_naive = list()\n",
    "reconstruction_error_naive = list()\n",
    "def naive_hook(module, input, output, quantization_grid=quantization_grid):\n",
    "    W = module.weight.clone()\n",
    "    quantized_weight = naive_quantization(W, quantization_grid=quantization_grid)\n",
    "    module.weight = nn.Parameter(quantized_weight)\n",
    "\n",
    "    module._forward_hooks.popitem()\n",
    "    new_output = module(*input)\n",
    "\n",
    "    weight_approximation_error_naive.append(F.mse_loss(W,module.weight))\n",
    "    reconstruction_error_naive.append(F.mse_loss(module.unquantized_output, new_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBC_hooks = []\n",
    "for layer_idx, layer in enumerate(model_OBC.bert.encoder.layer):\n",
    "    OBC_hooks.append(layer.attention.self.query.register_forward_hook(OBC_hook))\n",
    "    OBC_hooks.append(layer.attention.self.key.register_forward_hook(OBC_hook))\n",
    "    OBC_hooks.append(layer.attention.self.value.register_forward_hook(OBC_hook))\n",
    "    OBC_hooks.append(layer.attention.output.dense.register_forward_hook(OBC_hook))\n",
    "    OBC_hooks.append(layer.intermediate.dense.register_forward_hook(OBC_hook))\n",
    "    OBC_hooks.append(layer.output.dense.register_forward_hook(OBC_hook))\n",
    "\n",
    "naive_hooks = []\n",
    "for layer_idx, layer in enumerate(model_naive.bert.encoder.layer):\n",
    "    naive_hooks.append(layer.attention.self.query.register_forward_hook(naive_hook))\n",
    "    naive_hooks.append(layer.attention.self.key.register_forward_hook(naive_hook))\n",
    "    naive_hooks.append(layer.attention.self.value.register_forward_hook(naive_hook))\n",
    "    naive_hooks.append(layer.attention.output.dense.register_forward_hook(naive_hook))\n",
    "    naive_hooks.append(layer.intermediate.dense.register_forward_hook(naive_hook))\n",
    "    naive_hooks.append(layer.output.dense.register_forward_hook(naive_hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  print(\"Performing OBC quantization\")\n",
    "  outputs = model_OBC(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "  print(\"Performing naive quantization\")\n",
    "  outputs = model_naive(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_types = [\"weight_approximation_error_OBC\", \"reconstruction_error_OBC\", \"weight_approximation_error_naive\", \"reconstruction_error_naive\"]\n",
    "layer_idx = [0, 1, 2, 3]\n",
    "layer_types = [\"query\", \"key\", \"value\", \"proj\", \"intermediate\", \"output\"]\n",
    "\n",
    "\n",
    "results = dict()\n",
    "for result_type in result_types:\n",
    "    results[result_type] = dict()\n",
    "\n",
    "for layer_idx in layer_idx:\n",
    "    for result_type in result_types:\n",
    "        results[result_type]['layer_'+str(layer_idx)] = dict()    \n",
    "    for layer_type in layer_types:\n",
    "        results['weight_approximation_error_OBC']['layer_'+str(layer_idx)][layer_type] = weight_approximation_error_OBC.pop(0).item()\n",
    "        results['reconstruction_error_OBC']['layer_'+str(layer_idx)][layer_type] = reconstruction_error_OBC.pop(0).item()\n",
    "        results['weight_approximation_error_naive']['layer_'+str(layer_idx)][layer_type] = weight_approximation_error_naive.pop(0).item()\n",
    "        results['reconstruction_error_naive']['layer_'+str(layer_idx)][layer_type] = reconstruction_error_naive.pop(0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'unquantized': unquantized_model, 'OBC': model_OBC, 'naive': model_naive}\n",
    "\n",
    "for k,model in models.items():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "    results['loss_'+k] = avg_eval_loss\n",
    "    results['accuracy_'+k]= eval_accuracy\n",
    "\n",
    "    print(f\"Model type {k}, Eval Loss: {avg_eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unquantized_model.state_dict(), \"model_unquantized_weights_\"+str(quantization_grid)+\".pth\")\n",
    "torch.save(model_OBC.state_dict(), \"model_OBC_weights_\"+str(quantization_grid)+\".pth\")\n",
    "torch.save(model_naive.state_dict(), \"model_naive_weights_\"+str(quantization_grid)+\".pth\")\n",
    "\n",
    "torch.save(results, \"results_\"+str(quantization_grid)+\".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Hassibi, B., Stork, D. G., & Wolff, G. J. (1993, March). **Optimal brain surgeon and general network pruning**. In IEEE international conference on neural networks (pp. 293-299). IEEE.\n",
    "\n",
    "Frantar, E., & Alistarh, D. (2022). **Optimal brain compression: A framework for accurate post-training quantization and pruning**. Advances in Neural Information Processing Systems, 35, 4475-4488\n",
    "\n",
    "Frantar, E., Ashkboos, S., Hoefler, T., & Alistarh, D. (2022). **Gptq: Accurate post-training quantization for generative pre-trained transformers**. arXiv preprint arXiv:2210.17323.\n",
    "\n",
    "Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., ... & Liu, Q. (2019). **Tinybert: Distilling bert for natural language understanding.** arXiv preprint arXiv:1909.10351.\n",
    "\n",
    "Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2018). **GLUE: A multi-task benchmark and analysis platform for natural language understanding**. arXiv preprint arXiv:1804.07461."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
